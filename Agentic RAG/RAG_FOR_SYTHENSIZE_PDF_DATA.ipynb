{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fd9f653",
   "metadata": {},
   "source": [
    "Install required packages for the RAG system.\n",
    "\n",
    "1. Langgraph\n",
    "2. Langchain\n",
    "3. Qdrant\n",
    "4. Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5d21b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#Configuration\n",
    "DOCS_DIR = \"docs\"  # Directory containing your PDF's files\n",
    "MARKDOWN_DIR = \"markdown\" # Directory containing the pdfs converted to markdown\n",
    "PARENT_STORE_PATH = \"parent_store\" # Directory for parent chunk JSON files\n",
    "CHILD_COLLECTION = \"document_child_chunks\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(DOCS_DIR, exist_ok=True)\n",
    "os.makedirs(MARKDOWN_DIR, exist_ok=True)\n",
    "os.makedirs(PARENT_STORE_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "152c85f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM Setup\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "llm = ChatOllama(model=\"gpt-oss:20b-cloud\",temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0304d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 18 files: 100%|██████████| 18/18 [00:01<00:00, 13.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# Embeddings: Set up a Hybrid Search approach:\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_qdrant.fastembed_sparse import FastEmbedSparse\n",
    "\n",
    "# Dense embeddings for semantic understanding\n",
    "dense_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Sparse embeddings for keyword matching\n",
    "sparse_embeddings = FastEmbedSparse(\n",
    "    model_name=\"Qdrant/bm25\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba30a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Database Setup \n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as qdrant_models\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_qdrant.qdrant import RetrievalMode\n",
    "\n",
    "\n",
    "# Initialize Client\n",
    "if os.path.exists(\"qdrant_db\"):\n",
    "    print(\"✓ Qdrant client already exists\")\n",
    "else:\n",
    "    client = QdrantClient(path=\"qdrant_db\")\n",
    "\n",
    "#Get Embedding Dimension\n",
    "embedding_dimension = len(dense_embeddings.embed_query(\"test\"))\n",
    "\n",
    "def ensure_collections(collection_name):\n",
    "    if not client.collection_exists(collection_name):\n",
    "        client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=qdrant_models.VectorParams(\n",
    "                size = embedding_dimension,\n",
    "                distance = qdrant_models.Distance.COSINE\n",
    "            ),\n",
    "            sparse_vectors_config={\n",
    "                \"sparse\": qdrant_models.SparseVectorParams()\n",
    "            },\n",
    "        )\n",
    "        print(f\"✓ Created collection: {collection_name}\")\n",
    "    else:\n",
    "        print(f\"✓ Collection {collection_name} already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64d24a3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DOCS_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m overwrite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m md_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     25\u001b[0m             pdf_to_markdown(pdf_path, output_dir)\n\u001b[0;32m---> 28\u001b[0m pdfs_to_markdowns(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mDOCS_DIR\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/*.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m)               \n",
      "\u001b[0;31mNameError\u001b[0m: name 'DOCS_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "# PDF TO MARKDOWN\n",
    "import os\n",
    "import pymupdf.layout\n",
    "import pymupdf4llm\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def pdf_to_markdown(pdf_path, output_dir):\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "    md = pymupdf4llm.to_markdown(doc, header=False, footer=False, page_separators=True, ignore_images=True, write_images=False, image_path=None)\n",
    "    md_cleaned = md.encode('utf-8', errors='surrogatepass').decode('utf-8', errors='ignore')\n",
    "    output_path = Path(output_dir) / Path(doc.name).stem\n",
    "    Path(output_path).with_suffix(\".md\").write_bytes(md_cleaned.encode('utf-8'))\n",
    "\n",
    "\n",
    "def pdfs_to_markdowns(path_pattern, overwrite: bool = False):\n",
    "    output_dir = Path(MARKDOWN_DIR)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for pdf_path in map(Path, glob.glob(path_pattern)):\n",
    "        md_path = (output_dir / pdf_path.stem).with_suffix(\".md\")\n",
    "        if overwrite or not md_path.exists():\n",
    "            pdf_to_markdown(pdf_path, output_dir)\n",
    "\n",
    "\n",
    "pdfs_to_markdowns(f\"{DOCS_DIR}/*.pdf\")               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999f01c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Indexing\n",
    "# Implementing Parent child chunking\n",
    "# Large context Window (Parent chunks) stored as JSON\n",
    "# Child Chunks should be stored in Qdrant\n",
    "# Merges small chunks and splits large ones for consistency\n",
    "# Creates bidirectional links between parent and child chunks\n",
    "\n",
    "\n",
    "# Chunking Strategy:\n",
    "\n",
    "# Split by Markdown headers (#, ##, ###)\n",
    "# Merge chunks smaller than 2000 characters\n",
    "# Split chunks larger than 10000 characters\n",
    "# Create child chunks (500 chars) from each parent\n",
    "# Store parent chunks in JSON files\n",
    "# Index child chunks in vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af66e75",
   "metadata": {},
   "source": [
    "The Core Problem: \"Search\" vs. \"Synthesis\"\n",
    "In a standard RAG system, you have to choose a chunk size (e.g., 1000 characters). You face a dilemma:\n",
    "\n",
    "Small Chunks are great for searching. They are specific, so the vector embedding represents a single clear idea. (e.g., \"The capital of France is Paris\").\n",
    "Large Chunks are great for generation. The LLM needs surrounding sentences to understand the nuance, tone, and full details to write a good answer.\n",
    "If you only use small chunks, the LLM lacks context. If you only use large chunks, the vector search becomes \"diluted\" (the embedding tries to represent too many topics at once) and you miss relevant logic.\n",
    "\n",
    "Why Your Strategy Solves This\n",
    "Your implementation acts as the perfect middle ground:\n",
    "\n",
    "1. High Precision Search (The Child Chunks)\n",
    "Why 500 chars? Vectors (like all-mpnet-base) work best on focused text. By keeping the search chunks small, you ensure that when a user asks a specific question, the system finds the exact paragraph containing the answer.\n",
    "Result: You get highly accurate search hits.\n",
    "2. High Quality Answers (The Parent Chunks)\n",
    "Why JSON/Parent Store? Once the search finds the \"Child,\" we don't just feed that tiny 500-char snippet to the LLM. Instead, we look up its \"Parent\"—the full 2,000–10,000 character section.\n",
    "Result: The LLM gets the full chapter or section. It sees not just the specific fact, but the introduction, the caveats, and the conclusion surrounding it. This prevents it from hallucinating or missing key details.\n",
    "3. Semantic Boundaries (Markdown Splitting)\n",
    "Why Headers (#)? arbitrary splitting (e.g., every 1000 chars) often cuts sentences within the middle or breaks a logical argument in half.\n",
    "Result: By splitting at Markdown headers, you respect the document's original structure. The \"Parent\" chunk becomes a logical unit (like \"Chapter 1: Installation\" or \"Section 2.3: Pricing\"), which makes much more sense to an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796bff79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
