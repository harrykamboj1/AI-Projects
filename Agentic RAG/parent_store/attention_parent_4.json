{
  "page_content": "## **3.3 Position-wise Feed-Forward Networks**  \nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\n\n## FFN( _x_ ) = max(0 _, xW_ 1 + _b_ 1) _W_ 2 + _b_ 2  \n**==> picture [13 x 10] intentionally omitted <==**  \nWhile the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is _d_ model = 512, and the inner-layer has dimensionality _dff_ = 2048.\n\n## **3.4 Embeddings and Softmax**  \nSimilarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension _d_ model. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [24]. In the embedding layers, we multiply those weights by _[√] d_ model.\n\n## **3.5 Positional Encoding**  \nSince our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the  \n--- end of page.page_number=5 ---  \nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. _n_ is the sequence length, _d_ is the representation dimension, _k_ is the kernel size of convolutions and _r_ the size of the neighborhood in restricted self-attention.  \n|Layer Type|Complexity per Layer|Sequential|Maximum Path Length|\n|---|---|---|---|\n|||Operations||\n|Self-Attention|_O_(_n_2 _· d_)|_O_(1)|_O_(1)|\n|Recurrent|_O_(_n · d_2)|_O_(_n_)|_O_(_n_)|\n|Convolutional|_O_(_k · n · d_2)|_O_(1)|_O_(_logk_(_n_))|\n|Self-Attention (restricted)|_O_(_r · n · d_)|_O_(1)|_O_(_n/r_)|  \nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension _d_ model as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [8].  \nIn this work, we use sine and cosine functions of different frequencies:  \n_PE_ ( _pos,_ 2 _i_ ) = _sin_ ( _pos/_ 10000[2] _[i/d]_[model] ) _PE_ ( _pos,_ 2 _i_ +1) = _cos_ ( _pos/_ 10000[2] _[i/d]_[model] )  \nwhere _pos_ is the position and _i_ is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2 _π_ to 10000 _·_ 2 _π_ . We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset _k_ , _PEpos_ + _k_ can be represented as a linear function of _PEpos_ .  \nWe also experimented with using learned positional embeddings [8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.",
  "metadata": {
    "H2": "**3.3 Position-wise Feed-Forward Networks** -> FFN( _x_ ) = max(0 _, xW_ 1 + _b_ 1) _W_ 2 + _b_ 2 -> **3.4 Embeddings and Softmax** -> **3.5 Positional Encoding**",
    "source": "attention.pdf",
    "parent_id": "attention_parent_4"
  }
}