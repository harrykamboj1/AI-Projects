{
  "page_content": "## **5 Training**  \nThis section describes the training regime for our models.\n\n## **5.1 Training Data and Batching**  \nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\n\n## **5.2 Hardware and Schedule**  \nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\n\n## **5.3 Optimizer**  \nWe used the Adam optimizer [17] with _β_ 1 = 0 _._ 9, _β_ 2 = 0 _._ 98 and _ϵ_ = 10 _[−]_[9] . We varied the learning rate over the course of training, according to the formula:  \n_lrate_ = _d[−]_ model[0] _[.]_[5] _[·]_[ min(] _[step]_[_] _[num][−]_[0] _[.]_[5] _[, step]_[_] _[num][ ·][ warmup]_[_] _[steps][−]_[1] _[.]_[5][)] (3)  \nThis corresponds to increasing the learning rate linearly for the first _warmup_  steps_ training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used _warmup_  steps_ = 4000.\n\n## **5.4 Regularization**  \nWe employ three types of regularization during training:  \n**Residual Dropout** We apply dropout [27] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of _Pdrop_ = 0 _._ 1.  \n--- end of page.page_number=7 ---  \nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.  \n|Model|BLEU<br>EN-DE<br>EN-FR|Training Cost (FLOPs)|\n|---|---|---|\n|||EN-DE<br>EN-FR|\n|ByteNet [15]<br>Deep-Att + PosUnk [32]<br>GNMT + RL [31]<br>ConvS2S [8]<br>MoE[26]|23.75<br>39.2<br>24.6<br>39.92<br>25.16<br>40.46<br>26.03<br>40.56|1_._0_ ·_ 1020<br>2_._3_ ·_ 1019<br>1_._4_ ·_ 1020<br>9_._6_ ·_ 1018<br>1_._5_ ·_ 1020<br>2_._0_ ·_ 1019<br>1_._2_ ·_ 1020|\n|Deep-Att + PosUnk Ensemble [32]<br>GNMT + RL Ensemble [31]<br>ConvS2S Ensemble[8]|40.4<br>26.30<br>41.16<br>26.36<br>**41.29**|8_._0_ ·_ 1020<br>1_._8_ ·_ 1020<br>1_._1_ ·_ 1021<br>7_._7_ ·_ 1019<br>1_._2_ ·_ 1021|\n|Transformer (base model)<br>Transformer (big)|27.3<br>38.1<br>**28.4**<br>**41.0**|**3****_._3****_ ·_ 1018**<br>2_._3_ ·_ 1019|  \n**Label Smoothing** During training, we employed label smoothing of value _ϵls_ = 0 _._ 1 [30]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.",
  "metadata": {
    "H2": "**5 Training** -> **5.1 Training Data and Batching** -> **5.2 Hardware and Schedule** -> **5.3 Optimizer** -> **5.4 Regularization**",
    "source": "attention.pdf",
    "parent_id": "attention_parent_6"
  }
}